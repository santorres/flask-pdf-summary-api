from flask import Flask, request, jsonify
import os
import pdfplumber
import openai
import glob2
import textwrap
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

# Function to open a file and read its content
def open_file(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as infile:
            return infile.read()
    except Exception as e:
        print(f"An error occurred while opening the file '{filepath}': {str(e)}")
        return None  # Return None if an error occurs

# Function to save content to a file
def save_file(filepath, content):
    try:
        with open(filepath, 'w', encoding='utf-8') as outfile:
            outfile.write(content)
    except Exception as e:
        print(f"An error occurred while saving the file '{filepath}': {str(e)}")
        return False  # Return False if an error occurs
    return True  # Return True if the file was saved successfully

# Function to convert PDFs to text
def convert_pdf2txt(src_dir, dest_dir):
    files = os.listdir(src_dir)
    files = [i for i in files if '.pdf' in i]
    for file in files:
        try:
            with pdfplumber.open(src_dir+file) as pdf:
                output = ''
                for page in pdf.pages:
                    output += page.extract_text()
                    output += '\n\nNEW PAGE\n\n'
                    print('Destination directory File Path:', dest_dir+file.replace('.pdf','.txt'))
                save_file(dest_dir+file.replace('.pdf','.txt'), output.strip())
        except Exception as oops:
            print("An error occurred:", oops, file)

# Function to interact with GPT-3 model
def gpt_3(prompt):
    chatbot = open_file('pdfbot.txt')

    messages = [
        {"role": "system", "content": chatbot},
        {"role": "user", "content": prompt},
    ]

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=0.7,
        frequency_penalty=0,
        presence_penalty=0,
    )
    text = response['choices'][0]['message']['content']
    return text

# Create a Flask application instance
app = Flask(__name__)

# Initialize rate limiting with flask-limiter
limiter = Limiter(
    app,
    default_limits=["3 per minute"],  # Adjust the rate limit as needed
)

# Define a route for the endpoint with rate limiting
@app.route('/pdfsummary', methods=['POST'])
@limiter.limit("3 per minute")  # Adjust the rate limit as needed
def pdfsummary():
    try:
        # Load the OpenAI API key
        openai.api_key = open_file('openai_key.txt')

        print('Request received', request)

        # Get the PDF files from the request (use the key 'pdfs')
        pdf_files = request.files.getlist('pdfs')
        print("PDF files:", pdf_files)

        # Save the PDF files to a directory
        pdf_dir = 'PDFs/'
        for pdf_file in pdf_files:
            with open(os.path.join(pdf_dir, pdf_file.filename), 'wb') as f:
                f.write(pdf_file.read())

        # Convert PDFs to text
        convert_pdf2txt('PDFs/', 'textPDFs/')

        # Get a list of all text files in the specified folder
        pathfolder = 'textPDFs/'
        files = glob2.glob(pathfolder + '*.txt')
        print("Text files:", files)

        # Initialize an empty string to store the contents of all the text files
        alltext = ""

        # After converting PDFs to text and reading the text files
        for file in files:
            with open(file, 'r', encoding='utf-8') as infile:
                alltext += infile.read()

        # Split the contents into chunks
        chunks = textwrap.wrap(alltext, 4000)

        # Before calling the GPT-3 model for summarization
        result = []

        # Batch processing: Process chunks in batches
        batch_size = 5  # You can adjust the batch size as needed
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batch_prompt = '\n'.join([open_file('pdfprompt.txt').replace('<<SUMMARY>>', chunk) for chunk in batch])
            batch_prompt = batch_prompt.encode(encoding='ASCII', errors='ignore').decode()

            batch_summary = gpt_3(batch_prompt)
            result.extend(batch_summary.split('\n'))  # Split by line to get individual summaries

            # Print the summaries generated by the GPT-3 model
            for summary in batch_summary.split('\n'):
                print("Generated summary:", summary)

        summary_content = '\n\n'.join(result)

        # ... (Remaining code)

        return jsonify(result), 200
    except Exception as e:
        print("An error occurred:", str(e))
        return jsonify({"error": str(e)}), 500

# Run the Flask application
if __name__ == '__main__':
    app.run(debug=True, port=8000)
